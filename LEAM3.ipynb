{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LEAM3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPg6s4I+7zx0JnrjHF9uNbP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mann2108/LEAM-gcolab/blob/master/LEAM3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eOa6VeKNVD2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "573b56e2-62c6-45f7-a59e-033627a58d16"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C81LxHNJNhUW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "649d9c55-3599-42e2-c133-5468c8850a18"
      },
      "source": [
        "!ls\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfkNCZoLNkVv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        },
        "outputId": "43cae858-64d9-43fb-83f1-4f10bffb52fb"
      },
      "source": [
        "!pip install tensorflow==1.7.0"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/13/eb2c71c340e2885feb5d562c337272d9ec1b300642b7298b10924e3db098/tensorflow-1.7.0-cp27-cp27mu-manylinux1_x86_64.whl (48.0MB)\n",
            "\u001b[K     |████████████████████████████████| 48.0MB 84kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (2.0.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (3.8.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (0.2.2)\n",
            "Collecting tensorboard<1.8.0,>=1.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/5b/18f50b69b8af42f93c47cd8bf53337347bc1974480a10de51fdd7f8fd48b/tensorboard-1.7.0-py2-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 43.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (0.7.1)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (1.0.post1)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (1.1.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (1.16.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (0.8.0)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow==1.7.0) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.7.0) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.7.0) (5.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.4.0->tensorflow==1.7.0) (44.1.1)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 42.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.0) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.0) (3.1.1)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp27-none-any.whl size=107221 sha256=25632ac148dd2c124022db5896be011514763d5da33571e563f1cb265870b788\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "\u001b[31mERROR: fastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: html5lib, bleach, tensorboard, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.0\n",
            "    Uninstalling bleach-3.1.0:\n",
            "      Successfully uninstalled bleach-3.1.0\n",
            "  Found existing installation: tensorboard 2.1.0\n",
            "    Uninstalling tensorboard-2.1.0:\n",
            "      Successfully uninstalled tensorboard-2.1.0\n",
            "  Found existing installation: tensorflow 2.1.0\n",
            "    Uninstalling tensorflow-2.1.0:\n",
            "      Successfully uninstalled tensorflow-2.1.0\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-1.7.0 tensorflow-1.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-OAodT0Nxzy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54b7b3cd-5088-474e-90c3-0dc7b2c26be9"
      },
      "source": [
        "!pip --version"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pip 19.3.1 from /usr/local/lib/python2.7/dist-packages/pip (python 2.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO2gtWEXOHWb",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "d92fad34-d6d7-45ea-bd08-082aec83992c"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-96291a17-e73b-41f9-b262-6e3ceefd872a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-96291a17-e73b-41f9-b262-6e3ceefd872a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving LEAM-master.zip to LEAM-master.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw4pXTpuQKqB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a1aeb3f-7e21-40b1-c2d3-ae138dd4f321"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive\tLEAM-master.zip  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlRK6BYDQOYO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "outputId": "d7557ac7-9fe5-49f1-8747-ff1c5fe375f2"
      },
      "source": [
        "!unzip LEAM-master.zip"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  LEAM-master.zip\n",
            "bfd341ccd92009ab6b9e6b0578216f57bb2b921e\n",
            "   creating: LEAM-master/\n",
            "  inflating: LEAM-master/README.md   \n",
            "  inflating: LEAM-master/evaluation.py  \n",
            "  inflating: LEAM-master/generate_emb.py  \n",
            "  inflating: LEAM-master/glove_generate.py  \n",
            "  inflating: LEAM-master/main.py     \n",
            "  inflating: LEAM-master/main_multiclass.py  \n",
            "  inflating: LEAM-master/model.py    \n",
            "   creating: LEAM-master/plots/\n",
            "   creating: LEAM-master/plots/convergence_compare/\n",
            "  inflating: LEAM-master/plots/convergence_compare/Convergence_compare_plot.ipynb  \n",
            "  inflating: LEAM-master/plots/convergence_compare/convergence_compare.npz  \n",
            "  inflating: LEAM-master/plots/convergence_compare/filter_size.pdf  \n",
            "  inflating: LEAM-master/plots/convergence_compare/filter_size_compare.ipynb  \n",
            "  inflating: LEAM-master/plots/convergence_compare/yahoo_iteration.pdf  \n",
            "   creating: LEAM-master/plots/correlation/\n",
            "  inflating: LEAM-master/plots/correlation/Covariance_matrix_yahoo.pdf  \n",
            "  inflating: LEAM-master/plots/correlation/Covariance_matrix_yahoo_label.pdf  \n",
            " extracting: LEAM-master/plots/correlation/covariance.npy  \n",
            "  inflating: LEAM-master/plots/correlation/paper_plot_covariance_yahoo.ipynb  \n",
            "  inflating: LEAM-master/plots/correlation/yahoo_legend.pdf  \n",
            "  inflating: LEAM-master/plots/correlation/yahoo_tsne.pdf  \n",
            "  inflating: LEAM-master/plots/correlation/yahoo_tsne2.pdf  \n",
            "   creating: LEAM-master/plots/schemes/\n",
            "  inflating: LEAM-master/plots/schemes/scheme_a.pdf  \n",
            "  inflating: LEAM-master/plots/schemes/scheme_a.png  \n",
            "  inflating: LEAM-master/plots/schemes/scheme_b.pdf  \n",
            "  inflating: LEAM-master/plots/schemes/scheme_b.png  \n",
            "   creating: LEAM-master/plots/text_attention/\n",
            "  inflating: LEAM-master/plots/text_attention/attention plot.ipynb  \n",
            "  inflating: LEAM-master/plots/text_attention/yahoo_examples_1.npz  \n",
            "   creating: LEAM-master/plots/yahoo_partial/\n",
            "  inflating: LEAM-master/plots/yahoo_partial/SWEN_plot.ipynb  \n",
            "  inflating: LEAM-master/plots/yahoo_partial/yahoo_partial.pdf  \n",
            "  inflating: LEAM-master/preprocess_yahoo.py  \n",
            "  inflating: LEAM-master/utils.py    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsWzDBnoQWIZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "31ec5547-0083-4b83-eb4e-a673011b1b03"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltFQawRARK61",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b27b0f2f-6ead-4e6b-968b-51c316bec36f"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive\tLEAM-master  LEAM-master.zip  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT8dacjbRQL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv LEAM-master/* ."
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGpPUhBlRbQz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "8134bfd6-598a-42be-b13a-7ffcf2201980"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluation.py\t   LEAM-master\t       model.py\t\t    sample_data\n",
            "gdrive\t\t   LEAM-master.zip     plots\t\t    utils.py\n",
            "generate_emb.py    main_multiclass.py  preprocess_yahoo.py\n",
            "glove_generate.py  main.py\t       README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_FAjD1iRczk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "0dae7bb3-df28-4e27-8e8b-35c5671b3341"
      },
      "source": [
        "import os, sys, cPickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib import layers\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "from math import floor\n",
        "\n",
        "from model import *\n",
        "from utils import get_minibatches_idx, restore_from_save, tensors_key_in_file, prepare_data_for_emb, load_class_embedding"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use the retry module or similar alternatives.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r89IuwOYRgDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Options(object):\n",
        "    def __init__(self):\n",
        "        self.GPUID = 0\n",
        "        self.dataset = 'agnews'\n",
        "        self.fix_emb = True\n",
        "        self.restore = False\n",
        "        self.W_emb = None\n",
        "        self.W_class_emb = None\n",
        "        self.maxlen = 305\n",
        "        self.n_words = None\n",
        "        self.embed_size = 300\n",
        "        self.lr = 1e-3\n",
        "        self.batch_size = 100\n",
        "        # self.max_epochs = 20\n",
        "        self.max_epochs = 1\n",
        "        self.dropout = 0.5\n",
        "        self.part_data = False\n",
        "        self.portion = 1.0 \n",
        "        self.save_path = \"./save/\"\n",
        "        self.log_path = \"./log/\"\n",
        "        self.print_freq = 100\n",
        "        self.valid_freq = 100\n",
        "\n",
        "        self.optimizer = 'Adam'\n",
        "        self.clip_grad = None\n",
        "        self.class_penalty = 1.0\n",
        "        self.ngram = 55\n",
        "        self.H_dis = 300\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        for attr, value in self.__dict__.iteritems():\n",
        "            yield attr, value\n",
        "\n",
        "def emb_classifier(x, x_mask, y, dropout, opt, class_penalty):\n",
        "    # comment notation\n",
        "    #  b: batch size, s: sequence length, e: embedding dim, c : num of class\n",
        "    x_emb, W_norm = embedding(x, opt)  #  b * s * e\n",
        "    x_emb=tf.cast(x_emb,tf.float32)\n",
        "    W_norm=tf.cast(W_norm,tf.float32)\n",
        "    y_pos = tf.argmax(y, -1)\n",
        "    y_emb, W_class = embedding_class(y_pos, opt, 'class_emb') # b * e, c * e\n",
        "    y_emb=tf.cast(y_emb,tf.float32)\n",
        "    W_class=tf.cast(W_class,tf.float32)\n",
        "    W_class_tran = tf.transpose(W_class, [1,0]) # e * c\n",
        "    x_emb = tf.expand_dims(x_emb, 3)  # b * s * e * 1\n",
        "    H_enc = att_emb_ngram_encoder_maxout(x_emb, x_mask, W_class, W_class_tran, opt)\n",
        "    H_enc = tf.squeeze(H_enc)\n",
        "    # H_enc=tf.cast(H_enc,tf.float32)\n",
        "    logits = discriminator_2layer(H_enc, opt, dropout, prefix='classify_', num_outputs=opt.num_class, is_reuse=False)  # b * c\n",
        "    logits_class = discriminator_2layer(W_class, opt, dropout, prefix='classify_', num_outputs=opt.num_class, is_reuse=True)\n",
        "    prob = tf.nn.softmax(logits)\n",
        "    class_y = tf.constant(name='class_y', shape=[opt.num_class, opt.num_class], dtype=tf.float32, value=np.identity(opt.num_class),)\n",
        "    correct_prediction = tf.equal(tf.argmax(prob, 1), tf.argmax(y, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)) + class_penalty * tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=class_y, logits=logits_class))\n",
        "\n",
        "    global_step = tf.Variable(0, trainable=False)\n",
        "    train_op = layers.optimize_loss(\n",
        "        loss,\n",
        "        global_step=global_step,\n",
        "        optimizer=opt.optimizer,\n",
        "        learning_rate=opt.lr)\n",
        "\n",
        "    return accuracy, loss, train_op, W_norm, global_step"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie0M1DUgSKAB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1105f361-d8cf-47d1-853c-1a006246bafb"
      },
      "source": [
        "!ls \"/content/gdrive/My Drive/Colab Notebooks/data\""
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ag_news_glove.p  ag_news.p\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6dOXd4XRoHS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1a0ca3ca-d1cc-4253-dca2-cc9600c72a8a"
      },
      "source": [
        "def main():\n",
        "    # Prepare training and testing data\n",
        "    opt = Options()\n",
        "    # load data\n",
        "    if opt.dataset == 'yahoo':\n",
        "        loadpath = \"./data/yahoo.p\"\n",
        "        embpath = \"./data/yahoo_glove.p\"\n",
        "        opt.num_class = 10\n",
        "        opt.class_name = ['Society Culture',\n",
        "            'Science Mathematics',\n",
        "            'Health' ,\n",
        "            'Education Reference' ,\n",
        "            'Computers Internet' ,\n",
        "            'Sports' ,\n",
        "            'Business Finance' ,\n",
        "            'Entertainment Music' ,\n",
        "            'Family Relationships' ,\n",
        "            'Politics Government']\n",
        "    elif opt.dataset == 'agnews':\n",
        "        loadpath = \"/content/gdrive/My Drive/Colab Notebooks/data/ag_news.p\"\n",
        "        embpath = \"/content/gdrive/My Drive/Colab Notebooks/data/ag_news_glove.p\"\n",
        "        opt.num_class = 4\n",
        "        opt.class_name = ['World',\n",
        "                        'Sports',\n",
        "                        'Business',\n",
        "                        'Science']    \n",
        "    elif opt.dataset == 'dbpedia':\n",
        "        loadpath = \"./data/dbpedia.p\"\n",
        "        embpath = \"./data/dbpedia_glove.p\"\n",
        "        opt.num_class = 14\n",
        "        opt.class_name = ['Company',\n",
        "            'Educational Institution',\n",
        "            'Artist',\n",
        "            'Athlete',\n",
        "            'Office Holder',\n",
        "            'Mean Of Transportation',\n",
        "            'Building',\n",
        "            'Natural Place',\n",
        "            'Village',\n",
        "            'Animal',\n",
        "            'Plant',\n",
        "            'Album',\n",
        "            'Film',\n",
        "            'Written Work',\n",
        "            ]\n",
        "    elif opt.dataset == 'yelp_full':\n",
        "        loadpath = \"./data/yelp_full.p\"\n",
        "        embpath = \"./data/yelp_full_glove.p\"\n",
        "        opt.num_class = 5\n",
        "        opt.class_name = ['worst',\n",
        "                        'bad',\n",
        "                        'middle',\n",
        "                        'good',\n",
        "                        'best']\n",
        "    x = cPickle.load(open(loadpath, \"rb\"))\n",
        "    train, val, test = x[0], x[1], x[2]\n",
        "    train_lab, val_lab, test_lab = x[3], x[4], x[5]\n",
        "    wordtoix, ixtoword = x[6], x[7]\n",
        "    del x\n",
        "    print(\"load data finished\")\n",
        "\n",
        "    train_lab = np.array(train_lab, dtype='float32')\n",
        "    val_lab = np.array(val_lab, dtype='float32')\n",
        "    test_lab = np.array(test_lab, dtype='float32')    \n",
        "    opt.n_words = len(ixtoword)\n",
        "    if opt.part_data:\n",
        "        #np.random.seed(123)\n",
        "        train_ind = np.random.choice(len(train), int(len(train)*opt.portion), replace=False)\n",
        "        train = [train[t] for t in train_ind]\n",
        "        train_lab = [train_lab[t] for t in train_ind]\n",
        "    \n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = str(opt.GPUID)\n",
        "\n",
        "    print(dict(opt))\n",
        "    print('Total words: %d' % opt.n_words)\n",
        "\n",
        "    try:\n",
        "        opt.W_emb = np.array(cPickle.load(open(embpath, 'rb')),dtype='float32')\n",
        "        opt.W_class_emb =  load_class_embedding( wordtoix, opt)\n",
        "    except IOError:\n",
        "        print('No embedding file found.')\n",
        "        opt.fix_emb = False\n",
        "\n",
        "    with tf.device('/gpu:1'):\n",
        "        x_ = tf.placeholder(tf.int32, shape=[opt.batch_size, opt.maxlen],name='x_')\n",
        "        x_mask_ = tf.placeholder(tf.float32, shape=[opt.batch_size, opt.maxlen],name='x_mask_')\n",
        "        keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
        "        y_ = tf.placeholder(tf.float32, shape=[opt.batch_size, opt.num_class],name='y_')\n",
        "        class_penalty_ = tf.placeholder(tf.float32, shape=())\n",
        "        accuracy_, loss_, train_op, W_norm_, global_step = emb_classifier(x_, x_mask_, y_, keep_prob, opt, class_penalty_)\n",
        "    uidx = 0\n",
        "    max_val_accuracy = 0.\n",
        "    max_test_accuracy = 0.\n",
        "\n",
        "    config = tf.ConfigProto(log_device_placement=False, allow_soft_placement=True, )\n",
        "    config.gpu_options.allow_growth = True\n",
        "    np.set_printoptions(precision=3)\n",
        "    np.set_printoptions(threshold=np.inf)\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    with tf.Session(config=config) as sess:\n",
        "        train_writer = tf.summary.FileWriter(opt.log_path + '/train', sess.graph)\n",
        "        test_writer = tf.summary.FileWriter(opt.log_path + '/test', sess.graph)\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        if opt.restore:\n",
        "            try:\n",
        "                t_vars = tf.trainable_variables()\n",
        "                save_keys = tensors_key_in_file(opt.save_path)\n",
        "                ss = set([var.name for var in t_vars]) & set([s + \":0\" for s in save_keys.keys()])\n",
        "                cc = {var.name: var for var in t_vars}\n",
        "                # only restore variables with correct shape\n",
        "                ss_right_shape = set([s for s in ss if cc[s].get_shape() == save_keys[s[:-2]]])\n",
        "\n",
        "                loader = tf.train.Saver(var_list=[var for var in t_vars if var.name in ss_right_shape])\n",
        "                loader.restore(sess, opt.save_path)\n",
        "\n",
        "                print(\"Loading variables from '%s'.\" % opt.save_path)\n",
        "                print(\"Loaded variables:\" + str(ss))\n",
        "\n",
        "            except:\n",
        "                print(\"No saving session, using random initialization\")\n",
        "                sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        try:\n",
        "            for epoch in range(opt.max_epochs):\n",
        "                print(\"Starting epoch %d\" % epoch)\n",
        "                kf = get_minibatches_idx(len(train), opt.batch_size, shuffle=True)\n",
        "                for _, train_index in kf:\n",
        "                    uidx += 1\n",
        "                    sents = [train[t] for t in train_index]\n",
        "                    x_labels = [train_lab[t] for t in train_index]\n",
        "                    x_labels = np.array(x_labels)\n",
        "                    x_labels = x_labels.reshape((len(x_labels), opt.num_class))\n",
        "\n",
        "                    x_batch, x_batch_mask = prepare_data_for_emb(sents, opt)\n",
        "                    _, loss, step,  = sess.run([train_op, loss_, global_step], feed_dict={x_: x_batch, x_mask_: x_batch_mask, y_: x_labels, keep_prob: opt.dropout, class_penalty_:opt.class_penalty})\n",
        "\n",
        "                    if uidx % opt.valid_freq == 0:\n",
        "                        train_correct = 0.0\n",
        "                        # sample evaluate accuaccy on 500 sample data\n",
        "                        kf_train = get_minibatches_idx(500, opt.batch_size, shuffle=True)\n",
        "                        for _, train_index in kf_train:\n",
        "                            train_sents = [train[t] for t in train_index]\n",
        "                            train_labels = [train_lab[t] for t in train_index]\n",
        "                            train_labels = np.array(train_labels)\n",
        "                            train_labels = train_labels.reshape((len(train_labels), opt.num_class))\n",
        "                            x_train_batch, x_train_batch_mask = prepare_data_for_emb(train_sents, opt)  \n",
        "                            train_accuracy = sess.run(accuracy_, feed_dict={x_: x_train_batch, x_mask_: x_train_batch_mask, y_: train_labels, keep_prob: 1.0, class_penalty_:0.0})\n",
        "\n",
        "                            train_correct += train_accuracy * len(train_index)\n",
        "\n",
        "                        train_accuracy = train_correct / 500\n",
        "\n",
        "                        print(\"Iteration %d: Training loss %f \" % (uidx, loss))\n",
        "                        print(\"Train accuracy %f \" % train_accuracy)\n",
        "\n",
        "                        val_correct = 0.0\n",
        "                        kf_val = get_minibatches_idx(len(val), opt.batch_size, shuffle=True)\n",
        "                        for _, val_index in kf_val:\n",
        "                            val_sents = [val[t] for t in val_index]\n",
        "                            val_labels = [val_lab[t] for t in val_index]\n",
        "                            val_labels = np.array(val_labels)\n",
        "                            val_labels = val_labels.reshape((len(val_labels), opt.num_class))\n",
        "                            x_val_batch, x_val_batch_mask = prepare_data_for_emb(val_sents, opt)\n",
        "                            val_accuracy = sess.run(accuracy_, feed_dict={x_: x_val_batch, x_mask_: x_val_batch_mask,\n",
        "                                y_: val_labels, keep_prob: 1.0,\n",
        "                                class_penalty_:0.0                                         })\n",
        "\n",
        "                            val_correct += val_accuracy * len(val_index)\n",
        "\n",
        "                        val_accuracy = val_correct / len(val)\n",
        "                        print(\"Validation accuracy %f \" % val_accuracy)\n",
        "\n",
        "                        if val_accuracy > max_val_accuracy:\n",
        "                            max_val_accuracy = val_accuracy\n",
        "\n",
        "                            test_correct = 0.0\n",
        "                            \n",
        "                            kf_test = get_minibatches_idx(len(test), opt.batch_size, shuffle=True)\n",
        "                            for _, test_index in kf_test:\n",
        "                                test_sents = [test[t] for t in test_index]\n",
        "                                test_labels = [test_lab[t] for t in test_index]\n",
        "                                test_labels = np.array(test_labels)\n",
        "                                test_labels = test_labels.reshape((len(test_labels), opt.num_class))\n",
        "                                x_test_batch, x_test_batch_mask = prepare_data_for_emb(test_sents, opt)\n",
        "\n",
        "                                test_accuracy = sess.run(accuracy_,feed_dict={x_: x_test_batch, x_mask_: x_test_batch_mask,y_: test_labels, keep_prob: 1.0, class_penalty_: 0.0})\n",
        "\n",
        "                                test_correct += test_accuracy * len(test_index)                                \n",
        "                            test_accuracy = test_correct / len(test)\n",
        "                            print(\"Test accuracy %f \" % test_accuracy)\n",
        "                            max_test_accuracy = test_accuracy\n",
        "\n",
        "                print(\"Epoch %d: Max Test accuracy %f\" % (epoch, max_test_accuracy))\n",
        "                saver.save(sess, opt.save_path, global_step=epoch)\n",
        "            print(\"Max Test accuracy %f \" % max_test_accuracy)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print('Training interupted')\n",
        "            print(\"Max Test accuracy %f \" % max_test_accuracy)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load data finished\n",
            "{'restore': False, 'fix_emb': True, 'log_path': './log/', 'dataset': 'agnews', 'part_data': False, 'max_epochs': 1, 'clip_grad': None, 'embed_size': 300, 'class_name': ['World', 'Sports', 'Business', 'Science'], 'lr': 0.001, 'save_path': './save/', 'W_class_emb': None, 'class_penalty': 1.0, 'optimizer': 'Adam', 'valid_freq': 100, 'dropout': 0.5, 'batch_size': 100, 'ngram': 55, 'GPUID': 0, 'n_words': 13010, 'H_dis': 300, 'W_emb': None, 'num_class': 4, 'print_freq': 100, 'maxlen': 305, 'portion': 1.0}\n",
            "Total words: 13010\n",
            "load class embedding\n",
            "initialize word embedding finished\n",
            "initialize class embedding finished\n",
            "WARNING:tensorflow:From model.py:46: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`NHWC` for data_format is deprecated, use `NWC` instead\n",
            "WARNING:tensorflow:From model.py:52: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From model.py:164: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From <ipython-input-16-e8f9a41d91dd>:57: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
            "\n",
            "Starting epoch 0\n",
            "Iteration 100: Training loss 0.674657 \n",
            "Train accuracy 0.776000 \n",
            "Validation accuracy 0.853000 \n",
            "Test accuracy 0.852237 \n",
            "Iteration 200: Training loss 0.397759 \n",
            "Train accuracy 0.734000 \n",
            "Validation accuracy 0.880400 \n",
            "Test accuracy 0.875000 \n",
            "Iteration 300: Training loss 0.266983 \n",
            "Train accuracy 0.788000 \n",
            "Validation accuracy 0.895500 \n",
            "Test accuracy 0.892763 \n",
            "Iteration 400: Training loss 0.342543 \n",
            "Train accuracy 0.782000 \n",
            "Validation accuracy 0.902700 \n",
            "Test accuracy 0.895526 \n",
            "Iteration 500: Training loss 0.478335 \n",
            "Train accuracy 0.816000 \n",
            "Validation accuracy 0.904600 \n",
            "Test accuracy 0.903289 \n",
            "Iteration 600: Training loss 0.324513 \n",
            "Train accuracy 0.816000 \n",
            "Validation accuracy 0.910600 \n",
            "Test accuracy 0.905921 \n",
            "Iteration 700: Training loss 0.500084 \n",
            "Train accuracy 0.816000 \n",
            "Validation accuracy 0.911300 \n",
            "Test accuracy 0.909474 \n",
            "Iteration 800: Training loss 0.225975 \n",
            "Train accuracy 0.844000 \n",
            "Validation accuracy 0.912100 \n",
            "Test accuracy 0.912763 \n",
            "Iteration 900: Training loss 0.347381 \n",
            "Train accuracy 0.848000 \n",
            "Validation accuracy 0.911200 \n",
            "Iteration 1000: Training loss 0.337797 \n",
            "Train accuracy 0.864000 \n",
            "Validation accuracy 0.912500 \n",
            "Test accuracy 0.914211 \n",
            "Iteration 1100: Training loss 0.222247 \n",
            "Train accuracy 0.850000 \n",
            "Validation accuracy 0.915000 \n",
            "Test accuracy 0.915658 \n",
            "Epoch 0: Max Test accuracy 0.915658\n",
            "Max Test accuracy 0.915658 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI4yCZ2ASk9z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "16fd67ae-d867-47a4-b644-2824aaea9d04"
      },
      "source": [
        "!ls "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluation.py\t   LEAM-master.zip     model.pyc\t    save\n",
            "gdrive\t\t   log\t\t       plots\t\t    utils.py\n",
            "generate_emb.py    main_multiclass.py  preprocess_yahoo.py  utils.pyc\n",
            "glove_generate.py  main.py\t       README.md\n",
            "LEAM-master\t   model.py\t       sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiDYt6o5qGHj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5feb8cb5-11ec-41c7-e0f0-706e5713cd59"
      },
      "source": [
        "!ls save\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.data-00000-of-00001\t-0.index  -0.meta  checkpoint\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maek7sC1qIlb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1cef5b4e-ed3e-452d-ae22-a5718d12d902"
      },
      "source": [
        "!ls checkpoint"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access 'checkpoint': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNUKjm5sqMmB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e888c9d-0646-48c7-dda0-66288524f1dd"
      },
      "source": [
        "!ls save/checkpoint"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "save/checkpoint\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDTClibjqROs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}